{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edbe056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2 \n",
    "import os\n",
    "import numpy as np   \n",
    "import imgaug.augmenters as iaa\n",
    "\n",
    "# Define a sequence of augmentations\n",
    "\n",
    "augmentations = iaa.Sequential([\n",
    "    iaa.Fliplr(0.5),            \n",
    "    iaa.Affine(\n",
    "        rotate=(-20, 20),        \n",
    "        scale=(0.9, 1.1)         \n",
    "    ),\n",
    "    iaa.Multiply((0.8, 1.2)),    \n",
    "    iaa.GaussianBlur(sigma=(0, 1.0))  \n",
    "])\n",
    "\n",
    "\n",
    "# Initialize Mediapipe Pose model\n",
    "\n",
    "mp_pose = mp.solutions.pose            \n",
    "pose = mp_pose.Pose()                  \n",
    "\n",
    "# Function to extract 33 pose landmarks from one image\n",
    "\n",
    "def extract_landmarks(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Image not found: {image_path}\")\n",
    "        return None\n",
    "    results = pose.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    if results.pose_landmarks:\n",
    "        return np.array([[lm.x, lm.y, lm.z] for lm in results.pose_landmarks.landmark]).flatten()\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "dataset_path = 'yoga_dataset'\n",
    "\n",
    "X = []  # landmarks\n",
    "y = []  # Class labels\n",
    "\n",
    "# Loop through pose folders\n",
    "\n",
    "for pose_label in os.listdir(dataset_path):       \n",
    "    pose_folder = os.path.join(dataset_path, pose_label)\n",
    "    if not os.path.isdir(pose_folder):\n",
    "        continue\n",
    "\n",
    "    for image_file in os.listdir(pose_folder):\n",
    "        image_path = os.path.join(pose_folder, image_file)\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            print(f\"Image not found: {image_path}\")\n",
    "            continue\n",
    "\n",
    "        # Original image landmarks \n",
    "\n",
    "        landmarks = extract_landmarks(image_path)\n",
    "        if landmarks is not None:\n",
    "            X.append(landmarks)\n",
    "            y.append(pose_label)\n",
    "\n",
    "        # Apply N augmentations per image\n",
    "        \n",
    "        N = 3  \n",
    "        for _ in range(N):\n",
    "            aug_image = augmentations(image=image)\n",
    "            aug_results = pose.process(cv2.cvtColor(aug_image, cv2.COLOR_BGR2RGB))\n",
    "            if aug_results.pose_landmarks:\n",
    "                aug_landmarks = np.array([[lm.x, lm.y, lm.z] for lm in aug_results.pose_landmarks.landmark]).flatten()\n",
    "                X.append(aug_landmarks)\n",
    "                y.append(pose_label)\n",
    "\n",
    "\n",
    "print(f\"Total samples collected: {len(X)}\")\n",
    "print(\"Detected pose labels:\", set(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8508bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41830b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.971830985915493\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Chakrasana       1.00      0.95      0.98        22\n",
      "  Dhanurasana       0.92      1.00      0.96        11\n",
      "Padahastasana       0.92      1.00      0.96        11\n",
      "  Trikonasana       1.00      1.00      1.00        11\n",
      "  Vrikshasana       1.00      0.94      0.97        16\n",
      "\n",
      "     accuracy                           0.97        71\n",
      "    macro avg       0.97      0.98      0.97        71\n",
      " weighted avg       0.97      0.97      0.97        71\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7cc84e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and label encoder saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model to a file\n",
    "joblib.dump(model, 'yoga_pose_classifier.pkl') \n",
    "\n",
    "# Save the label encoder to a file\n",
    "joblib.dump(y, 'pose_label_encoder.pkl')\n",
    "\n",
    "print(\"Model and label encoder saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc4b6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load trained model and label encoder\n",
    "model = joblib.load('yoga_pose_classifier.pkl')\n",
    "label_encoder = joblib.load('pose_label_encoder.pkl')\n",
    "                                                            \n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "pose = mp_pose.Pose()\n",
    "                                                                      \n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "                                                                  \n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(image_rgb)\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "        # Extract landmark data\n",
    "\n",
    "        landmarks = np.array([[lm.x, lm.y, lm.z] for lm in results.pose_landmarks.landmark]).flatten().reshape(1, -1)\n",
    "\n",
    "        # To predict pose\n",
    "        \n",
    "        probs = model.predict_proba(landmarks)[0]\n",
    "        max_prob = np.max(probs)\n",
    "        pred_class = np.argmax(probs)\n",
    "\n",
    "        pose_name = model.classes_[pred_class] if max_prob > 0.70 else \"Unknown\"\n",
    "\n",
    "        # To display predicted pose\n",
    "\n",
    "        cv2.putText(frame, f'Pose: {pose_name}', (30, 50),cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('Yoga Pose Detection', frame)\n",
    "\n",
    "    # Press 'q' to quit\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
